---
title: "House Price Prediction"
author: Akoua Orsot
date: August 22, 2022
version: 1.0
output: github_document
---

# **House Price Predictions**

Different variables come into play in appraising a house, such as the number of bedrooms, square footage, location, and much more. So, our task here is to build a machine learning model to make reasonably accurate predictions in terms of pricing houses. It would be an opportunity for those in real estate to gain more visibility on the market as a whole. In doing so, this notebook will offer a user-friendly explanation through every step using LIME (Local Interpretable Model-agnostic Explanations) principles.

## **Table of Contents**

1.  Environment set-up

    -   Importing Libraries

    -   Loading the data

2.  Initial Diagnostics

    -   Glimpse

    -   Descriptive Statitics

    -   Target Variable Analysis

    -   Predictors Analysis

3.  Data Cleaning

    -   Missing Values

    -   Outliers

    -   Duplicate Observations

4.  Correlation Analysis

    -   Correlation Matrix

    -   Strongest relationship

5.  Inquiry Exploration

6.  Feature Engineering

7.  Machine Learning set-up

    -   Train-test split

    -   Cross-validation

8.  Feature Selection

9.  Dimensionality Reduction

10. Machine Learning - Simple Models

11. Machine Learning - Ensemble Methods

12. Hyperparameter Tuning

13. Model Performance Evaluation

14. Recommendation & Conclusion

## **1. Environment Set-up**

```{r}
## Importing libraries
set.seed(1)
library(dplyr)
library(tidyr)
library(tidyverse)
library(ggplot2)
library(ggcorrplot)    # Visualize correlation matrix
library(caret)
library(VIM)           # Visualize mising values
library(mice)          # missing data imputation
library(naniar)
library(stringr)

library(corrplot)
```

```{r}
## Loading dataset
df <- read_csv(file = 'C:/Users/Graduate/Desktop/ds_proj_data/house_price/train.csv')
df %>% head()
```

## 2. Initial Diagnostics

```{r}
## Info
df %>% str()
```

**Takeaway:** From the glimpse above, we could already draw some observations.

-   Our dataset comprises 1460 rows and 80 columns, making it relatively small, so we would not expect the training process o to be as computationally intensive.

-   For null values, most columns appear to have no missing values, while null values make up 80% for some of those variables. It indicates that we shall proceed with data cleaning and tidying before doing any statistical analysis or machine learning.

-   In terms of variable type, we have mostly int64, float64, and object. Though 'object' can indicate text or categorical, we will need to investigate further in feature engineering.

```{r}
## Descriptive Statistics
df %>% summary()
```

**Takeaway:** For all 80 variables, the table above captures the basic descriptive statistics showing things like mean, standard deviation, min, max, etc. Commenting on each variable would bring little value to our overall analysis, and so we will zoom on the target variable 'SalePrice'.

```{r}
# Stats for the target variable
df$SalePrice %>% summary()
```

**Takeaway:** The count indicates no null values in the column. The houses in the dataset vary from \~USD34.9k to \~USD755k, with a mean value of \~USD180k. With the standard deviation at \~USD79k, it appears that prices fluctuate pretty significantly, or we may potentially have houses with exorbitant prices (outliers) skewing the data. We will create a histogram to look at the distribution more closely.

```{r}
## Feature Variable Analysis
df %>% 
    ggplot(aes(x=SalePrice)) + 
    geom_histogram(bins = 50) +
    labs(title = "Distribution of House Sale Price",
          x = "Dollar Amount ($)", y = "Frequency (Count)")
```

**Takeaway:** From the histogram above, we can deduct that house sale prices in this dataset have a right-skewed distribution with outliers on the upper end, indicating luxury houses with higher price points. However, most houses appear to fall between \~USD100k and \~USD300k, relatively consistent with real estate markets in the United States.

## 

3.  Data Cleaning

```{r}
missvalues_visual <- 
    df  %>%
      summarise_all(list(~is.na(.)))%>%
      pivot_longer(everything(),
                   names_to = "variables", values_to="missing") %>%
      count(variables, missing) %>%
      ggplot(aes(y=variables,x=n,fill=missing))+
      geom_col()+
      scale_fill_manual(values=c("skyblue3","gold"))+
      theme(axis.title.y=element_blank())
missvalues_visual
options(repr.plot.width = 14, repr.plot.height = 5)
```

**Takeaway:** As the plot shows above, there are indeed null values confirming our observation in the initial diagnostics. Given that not all variables are of the same type or the same proportion of missing values, the cleaning process will attend to each column or group of similar columns.

**Definition:** When it comes to data science, we are constantly dealing with imperfect information, thus murking the waters on the quality of data overall. One of those issues is the recurrence of missing values and requires effective techniques to deal with them. Imputation methods present such an opportunity using strategies to replace null values with statistical measures like mean, mode, or median. More information [here](https://machinelearningmastery.com/statistical-imputation-for-missing-values-in-machine-learning/).

```{r}
## No. of null values
null_vals <- sum(is.na(df))

# List of columns with missing values
null_cols <- which(colSums(is.na(df))>0)

# Reporting back
sprintf(fmt="We are missing %d values in our data at given percentages in the following columns:\n",
       null_vals) %>% cat()

for (i in null_cols)
    {
    col_name <- names(df[, i])
    null_val <- sum(is.na(df[col_name]))
    null_per <- (null_val / nrow(df))*100
    sprintf(fmt = " -%s: %d (%.2f%%)\n", 
            col_name, null_val, null_per) %>% cat()
}
```

**LotFrontage:** As per the data dictionary, it is the linear feet of street connected to property. It indicates the measurement of a piece of land (lot) often defined by frontage and depth respectively. For instance, an house can be 50 by 150, meaning 50 feet wide (frontage) and 150 feet long. Read more about it [here](https://www.gimme-shelter.com/frontage-50043/). Given that 'LotFrontage' is one of those characteristics all houses have, the null values indicate missing information that cannot just be equal to 0. Since we cannot get back and fetch more data, we will use imputation methods for this column and other ones which may require them.

**Note:** Before proceeding to the imputation, we would like to investigate possible differences in distribution grouped by Lot shape.

```{r}
df %>% 
    ggplot(aes(x=LotFrontage)) +
    geom_boxplot(outlier.colour="red", outlier.shape=16,
     outlier.size=2) 
```

```{r}
df %>% 
    ggplot(aes(x=LotFrontage, y=LotShape)) +
    geom_boxplot(outlier.colour="red", outlier.shape=16,
     outlier.size=2) +
    facet_wrap("LotShape")
```

```{r}
sprintf(fmt = "For all houses' LotFrontage, the mean is %.2f and median is %.2f",
        mean(df$LotFrontage, na.rm=TRUE), median(df$LotFrontage, na.rm=TRUE)) %>% cat()
```

```{r}
sprintf("For: \n") %>% cat()
for (i in unique(df$LotShape))
    {
    df_i <- df %>% filter(LotShape==i)
    sprintf(
            fmt = " -%s houses, the mean LotFrontage is %.2f and median LotFrontage is %.2f\n",
            i, mean(df_i$LotFrontage, na.rm=TRUE), median(df_i$LotFrontage, na.rm=TRUE)
            ) %>% cat()
}
```

**Takeaway:** The boxplots indicate the presence of outliers in the data with massive and small houses by widths. When broken down by 'LotShape', we also observe a notable difference in those houses categorized as IR3, in other words, of very irregular shape. In light of both the outliers and category differences, we will use the median value grouped by LotShape for the imputation process to ensure consistency in the data.

```{r}
df <- df %>% 
  group_by(LotShape) %>% 
  mutate(LotFrontage = ifelse(is.na(LotFrontage), 
                            median(LotFrontage, na.rm = TRUE), 
                            LotFrontage))

df$LotFrontage %>% is.na() %>% sum()
```

**Alley:** As per the data dictionary, it refers to the type of alley access to property. Given the real estate market in question, it may affect the price more or less and so, the null values are indeed significant with NA indicating that there isn't one. To ensure that it is taken into account, we will rename the NA into the full phrase 'No alley access' and then proceed in encoding this categorical variable.

```{r}
df$Alley <- df$Alley %>% replace_na('No alley access')
df$Alley %>% is.na() %>% sum()
```

**Variable Grouping:** It appears that the process in detecting missing valuies actually led to understanding those null values are actually categories significant or equal to 0 per the data dictionary. So, to be more efficient, we will make a list of those columns and the term/value we'll use to replace the na values.

```{r}
for (i in names(df[, null_cols])) {
    # Grouping of variables dependent on the presence of a basement
    if (str_detect(i, "Bsmt") == TRUE) {
        df[, i][is.na(df[, i])] <- 'No Basement'
        
    # Grouping of variables dependent on the presence of a garage
    } else if (str_detect(i, "Garage") == TRUE) {
        if (i == 'GarageYrBlt'){
            df[, i][is.na(df[, i])] <- 0
        } else {
            df[, i][is.na(df[, i])] <- 'No Garage'
        }
        
    }
}
```

```{r}
# Grouping of variables dependent on the presence of other amenities
df$MasVnrType  <- df$MasVnrType %>% replace_na('No Veneer')
df$MasVnrArea  <- df$MasVnrArea %>% replace_na(0)
df$FireplaceQu <- df$FireplaceQu %>% replace_na('No Fireplace')
df$PoolQC <- df$PoolQC %>% replace_na('No Pool')
df$Fence <- df$Fence %>% replace_na('No Fence')
df$MiscFeature <- df$MiscFeature %>% replace_na('No Misc')
```

**Note:** Assuming all houses have an electrical system, we will drop the obersvation having the eltrical system as a null values.

```{r}
# Deleting the Electrical 
df <- drop_na(df, any_of("Electrical"))
```

```{r}
## No. of null values
null_vals <- df %>% is.na() %>% sum()

# Reporting back
sprintf(fmt="Afer imputation, we have missing %d values in our data.",
        null_vals) %>% cat()
```

## 

4.  Inquiry Exploration

In this section, we will generate various questions to further consolidate our understanding of the problem at hand. It will allow us to guide the machine learning process more attuned to the particular subject matter.

**Question 1:** Do bigger houses always translate into higher prices?

```{r}
## Scatterplot between lotArea and SalePrice
df %>% ggplot(aes(x=LotArea, y=SalePrice)) +
  geom_point()
```

**Takeaway:** From the scatterplot above, there is very little evidence indicating that bigger houses are ultimiately pricier. As noted in the diagnostics, the 80 initial variables show how the house valuation process is multi-dimensional.

**Question 2:** Where is the real estate hotspot?

```{r}
# Which neighborhood registers the most sales?
total <- df %>% 
            group_by(Neighborhood) %>%
            summarise(count = n_distinct(SalePrice)) %>% 
            arrange(desc(count)) %>%
            mutate(percent = count / sum(count) * 100)
            
total[1,] 
```

```{r}
# Which neighborhood registers the sales with the highest price tags?
avg <- df %>% 
            group_by(Neighborhood) %>%
            summarise(avg = mean(SalePrice)) %>% 
            arrange(desc(avg))
            
avg[1,] 
```

**Note:** As per the data dictionary, NAmes refers North Ames and NoRidge refers to Northridge both located in Iowa, US.

**Question 3:** What miscellaneous feature add the most value?

```{r}
# Which miscellaneous feature is the most prevalent?
misc <- df %>% 
            group_by(MiscFeature) %>%
            summarise(count = n_distinct(SalePrice)) %>% 
            arrange(desc(count))
            
misc[2,] 
```

**Takeaway:** For houses with miscellaneous features, Shed is the most prevalent in 46 houses.

```{r}
# Calculating the value added
misc_rows <- df %>% filter(MiscFeature == 'Shed')
avg_mon_val <- mean(misc_rows$MiscVal)
per_sale <- mean(misc_rows$MiscVal/misc_rows$SalePrice)*100
avg_mon_val
per_sale
```

**Takeaway:** Shed has on average \$697.84 of monetary value making up 0.48% of the house sale price on average.

## 5. Feature Engineering

**Feature Scaling:** When dealing with data, we are working with different types of which required adpated pre-processing before applying any machine learning techniques. In our content, we perform feature scaling to standardize only the values in continuous numerical variables. Read more [here](https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35).

```{r}
# Filter numeric columns
num_vars = c('LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1',
           'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',
           'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',
            'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal')

df[, num_vars] <- scale(df[, num_vars], center=TRUE, scale=TRUE)
df %>% head()
```

**Categorical feature encoding** ensures that variables with categories/groupings are transformed into numerical inputs for the predictive modeling phase. The categorical variables are also subdivided as:

-   binary (two possible outcomes)

-   cardinal (no meaningful order)

-   ordinal (meaningful order)

Read more [here](https://www.analyticsvidhya.com/blog/2020/08/types-of-categorical-data-encoding/).

```{r}
# List of nominal categorical variables
cat_vars = c('CentralAir', 'MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotConfig', 
            'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 
            'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 
            'Exterior2nd', 'MasVnrType', 'Foundation', 'Electrical', 
            'Functional', 'GarageType', 'MiscFeature', 'SaleType', 
            'SaleCondition', 'LotShape', 'LandContour', 'Utilities', 
            'LandSlope', 'OverallQual', 'OverallCond', 'ExterQual',
            'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure',
            'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual',
            'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond',
            'PavedDrive', 'PoolQC', 'Fence')

df[cat_vars] <- lapply(df[cat_vars], factor)
df %>% head()
```

**Datetime Variables:** There are variables denoting dates and thus, may hold significance and impact our target variable: the house's sale price.

Based on research, we thought that the most sensible option would be to transform the datetime variables into ordinal categories in twofold:

-   Direct encoding of 'MoSold' and 'YrSold' having 12 and 5 pre-defined categories that are the 12 months and 5 years respectively during which the houses in the dataset were sold.

-   Binning of 'YearRemodAdd' and 'YearBuilt' into 6 categories of 10 and 20 years of interval respectively before proceeding to ordinal encoding as well.

```{r}
df <- df %>% 
        mutate(YearRemodAdd = cut(YearRemodAdd, breaks=6),
               YearBuilt = cut(YearBuilt, breaks=6))
df %>% head()
```

```{r}
# List of date categorical variables
cat_vars = c('YearRemodAdd', 'YearBuilt', 'MoSold', 'YrSold')

df[cat_vars] <- lapply(df[cat_vars], factor)
df %>% head()
```

## 4. Correlation Analysis

```{r}
# Correlation matrix and plot
df_num <- df[, sapply(df, class) == "numeric"]
cor <- cor(df_num)
ggcorrplot(cor, hc.order = TRUE, insig = "blank",
           type = "lower", ggtheme = theme_gray,
           colors = c("#6D9EC1", "white", "#E46726"),
           tl.cex = 10)
```

**Note:** Next, we will only filter out relatively and highly correlated relationship with coefficient between 0.7 and 1 (non-inclusive to avoid pairs of identical variables).

```{r}
findCorrelation(x=cor, cutoff = .7, names=TRUE)
```

## 7. Machine Learning Set-Up

Under this section, we will explain the procedure of two main splitting approach to estimate our models' performance.

```{r}
## Training Testing Split
N <- nrow(df)
trainingSize  <- round(N*0.7)
trainingCases <- sample(N, trainingSize)
train <- df[trainingCases,]
test <- df[-trainingCases,]
```

```{r}
# K-Fold Cross Validation
train.control <- trainControl(method = "cv", number = 10, verboseIter = FALSE)
# Train the model
model <- train(SalePrice ~., data = df, method = "lm",
               trControl = train.control)

# Making predictions
# pred <- predict(model, train)
# obs <- train$SalePrice
# rmse(obs, pred)
```

**Takeaway:** Based on the printout above, it will inform us about a multicollinearity issue we need to solve. So, before building any new model, we will test various techniques to solve it.
