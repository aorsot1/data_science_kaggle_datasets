---
title: "House Price Prediction"
author: Akoua Orsot
date: August 22, 2022
version: 1.0
output: github_document
---

# **House Price Predictions**

Different variables come into play in appraising a house, such as the number of bedrooms, square footage, location, and much more. So, our task here is to build a machine learning model to make reasonably accurate predictions in terms of pricing houses. It would be an opportunity for those in real estate to gain more visibility on the market as a whole. In doing so, this notebook will offer a user-friendly explanation through every step using LIME (Local Interpretable Model-agnostic Explanations) principles.

## **Table of Contents**

1\. Environment set-up

-   Importing Libraries

-   Loading the data

2\. Initial Diagnostics

-   Glimpse

-   Descriptive Statistics

-   Target Variable Analysis

-   Predictors Analysis

3\. Data Processing

-   Basic cleanup

-   Missing Values - Imputation

-   Outliers Detection

4\. Inquiry Exploration

-   Does bigger means pricier houses?

-   Where is the real estate hot spot?

-   Which miscellaneous feature add the most value?

5\. Feature Engineering

-   Datetime Variables

-   Categorical Encoding

<!-- -->

-   Outliers - Feature Scaling

6\. Correlation Analysis

7\. Machine Learning set-up

8\. Machine Learning - Model Building

9\. Hyper-parameter Tuning

10\. Final Submission

## **1. Environment Set-up**

```{r}
## Importing libraries
set.seed(1)
library(dplyr)
library(tidyr)
library(tidyverse)
library(ggplot2)
library(ggcorrplot)    # Visualize correlation matrix
library(caret)
library(VIM)           # Visualize mising values
library(mice)          # missing data imputation
library(naniar)
library(stringr)
library(data.table)
library(corrplot)
```

```{r}
## Loading dataset
df <- read_csv(file = 'C:/Users/Graduate/Desktop/ds_proj_data/house_price/train.csv')
df %>% head()
```

## 2. Initial Diagnostics

```{r}
## Info
df %>% str()
```

**Takeaway:** From the glimpse above, we could already draw some observations.

-   Our dataset comprises 1460 rows and 80 columns, making it relatively small, so we would not expect the training process o to be as computationally intensive.

-   For null values, most columns appear to have no missing values, while null values make up 80% for some of those variables. It indicates that we shall proceed with data cleaning and tidying before doing any statistical analysis or machine learning.

-   In terms of variable type, we have mostly int64, float64, and object. Though 'object' can indicate text or categorical, we will need to investigate further in feature engineering.

```{r}
## Descriptive Statistics
df %>% summary()
```

**Takeaway:** For all 80 variables, the table above captures the basic descriptive statistics showing things like mean, standard deviation, min, max, etc. Commenting on each variable would bring little value to our overall analysis, and so we will zoom on the target variable 'SalePrice'.

```{r}
# Stats for the target variable
df$SalePrice %>% summary()
```

**Takeaway:** The count indicates no null values in the column. The houses in the dataset vary from \~USD34.9k to \~USD755k, with a mean value of \~USD180k. With the standard deviation at \~USD79k, it appears that prices fluctuate pretty significantly, or we may potentially have houses with exorbitant prices (outliers) skewing the data. We will create a histogram to look at the distribution more closely.

```{r}
## Feature Variable Analysis
df %>% 
    ggplot(aes(x=SalePrice)) + 
    geom_histogram(bins = 50) +
    labs(title = "Distribution of House Sale Price",
          x = "Dollar Amount ($)", y = "Frequency (Count)")
```

**Takeaway:** From the histogram above, we can deduct that house sale prices in this dataset have a right-skewed distribution with outliers on the upper end, indicating luxury houses with higher price points. However, most houses appear to fall between \~USD100k and \~USD300k, relatively consistent with real estate markets in the United States.

## 3. Data Processing

```{r}
basic_processing <- function(data, col_drop, old_col_names, new_col_names){

  data <- data %>% dplyr::select(-all_of(col_drop))
  data <- setnames(data, old = old_col_names, 
         new = new_col_names)
  
  # if (length(col_drop) > 0){
  #   data <- data %>% dplyr::select(-all_of(col_drop))
  # }
  
  return(data)
}

df <- basic_processing(data=df, col_drop=c(), old_col_names=c("SalePrice"), 
                 new_col_names=c("target"))
```

**Note:** In data science, imperfect information is all we have so, there is always doubt on data quality of which, a recurrent issue is **missing values**. One could attempt collecting more data, delete the missing value, or proceed by imputation. Imputation methods offre replacement strategies by leveraging statistical measures like mean, mode, or median. More information [here](https://machinelearningmastery.com/statistical-imputation-for-missing-values-in-machine-learning/).

```{r}
identify_missing_val <- function(data) {
  ## No. of null values
  null_vals <- sum(is.na(data))
  
  # List of columns with missing values
  null_cols <- which(colSums(is.na(data))>0)
  
  # Reporting back
  sprintf(fmt="Those columns have missing values in those counts and proportions: %d:\n",
         null_vals) %>% cat()
  
  for (i in null_cols)
      {
      col_name <- names(data[, i])
      null_val <- sum(is.na(data[col_name]))
      null_per <- (null_val / nrow(data))*100
      sprintf(fmt = " -%s: %d (%.2f%%)\n", 
              col_name, null_val, null_per) %>% cat()
  }
}

identify_missing_val(df)
```

**Takeaway:** Having confirmed our observation in the initial diagnostics, the difference across the different varibles would require an adapted cleaning process attending to each column or group of similar columns.

As per the data dictionary, **LotFrontage** is the linear feet of street connected to property and indicates the measurement of a piece of land (lot) defined by frontage and depth respectively. For instance, an house can be 50 by 150, meaning 50 feet wide (frontage) and 150 feet long. Read more about it [here](https://www.gimme-shelter.com/frontage-50043/). Given that 'LotFrontage' is one of those characteristics all houses have, the null values indicate missing information that cannot just be equal to 0. Since we cannot go out and collect more data, we will investigate differences in distribution across Lot shape.

```{r}
sprintf(fmt = "For all houses' LotFrontage, the mean is %.2f and median is %.2f",
        mean(df$LotFrontage, na.rm=TRUE), median(df$LotFrontage, na.rm=TRUE)) %>% cat()

sprintf("For: \n") %>% cat()
for (i in unique(df$LotShape))
    {
    df_i <- df %>% filter(LotShape==i)
    sprintf(
            fmt = " -%s houses, the mean LotFrontage is %.2f and median LotFrontage is %.2f\n",
            i, mean(df_i$LotFrontage, na.rm=TRUE), median(df_i$LotFrontage, na.rm=TRUE)
            ) %>% cat()
}
```

**Takeaway:**

-   We observe a notable difference in houses categorized as IR3 (very irregular shape) thus, the imputation on ***LotFrontage*** will be done using median value grouped by ***LotShape*** to ensure consistency in the data.

-   For the other columns with missing information, the missing values refers the absence of certain amenities in a given home. So, we will replace those with a separate category as "No X Amenity"

-   Such as in the instance of "Electrical", we will just delete the observation(s) with the non-meaningful null values.

```{r}
missing_val_fix <-function(data, rep_set_1, rep_set_others){
  # Deleting the Electrical 
  #data <- drop_na(data, any_of("Electrical"))
  # data <- data %>% 
  #   group_by(LotShape) %>% 
  #   mutate(LotFrontage = ifelse(is.na(LotFrontage), 
  #                             median(LotFrontage, na.rm = TRUE), 
  #                             LotFrontage))

  # Grouping of variables dependent on the presence of other amenities
  #data <- data %>% replace_na(rep_set_1)
  
  for (name in names(rep_set_others)) {
    change_val <- rep_set_others[[name]][1]
    cols <- as.vector(rep_set_others[[name]][-1])
    #print(cols)
    # data <- data %>% mutate_at(vars(cols), ~replace_na(., change_val))
    data <- data %>% mutate(across(cols, ~replace_na(., change_val)))
  }
  
  # ## No. of null values
  # null_vals <- data %>% is.na() %>% sum()
  # 
  # # Reporting back
  # sprintf(fmt="Afer imputation, we have missing %d values in our data.",
  #         null_vals) %>% cat()
  
  #return(data)
  
}
```

```{r}
rep_set_1 <- list('Alley'='No Alley', 'FireplaceQu'='No Fireplace',
            'PoolQC'='No Pool', 'Fence'='No Fence','MiscFeature'='No Misc')

rep_set_others <- list(
  "set_2" = c("Unknown", 'MSZoning', 'Utilities', 'Exterior1st', 
              'Exterior2nd', 'MasVnrType', 'KitchenQual',
              'Functional', 'SaleType'),
  
  "set_3" = c("No Basement", 'BsmtQual', 'BsmtCond', 
              'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'),
  
  "set_4" = c("No Garage", 'GarageType', 'GarageFinish', 
              'GarageQual', 'GarageCond'),
  
  "set_5" = c(0, 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 
              'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 
              'BsmtHalfBath', 'GarageCars', 'GarageYrBlt',
              'GarageArea')
)


#missing_val_fix(df, rep_set_1, rep_set_others)
```

4.  -Inquiry Exploration

In this section, we will generate various questions to further consolidate our understanding of the problem at hand. It will allow us to guide the machine learning process more attuned to the particular subject matter.

**Question 1:** Do bigger houses always translate into higher prices?

```{r}
## Scatterplot between lotArea and SalePrice
df %>% ggplot(aes(x=LotArea, y=SalePrice)) +
  geom_point()
```

**Takeaway:** From the scatterplot above, there is very little evidence indicating that bigger houses are ultimiately pricier. As noted in the diagnostics, the 80 initial variables show how the house valuation process is multi-dimensional.

**Question 2:** Where is the real estate hotspot?

```{r}
# Which neighborhood registers the most sales?
total <- df %>% 
            group_by(Neighborhood) %>%
            summarise(count = n_distinct(SalePrice)) %>% 
            arrange(desc(count)) %>%
            mutate(percent = count / sum(count) * 100)
            
total[1,] 
```

```{r}
# Which neighborhood registers the sales with the highest price tags?
avg <- df %>% 
            group_by(Neighborhood) %>%
            summarise(avg = mean(SalePrice)) %>% 
            arrange(desc(avg))
            
avg[1,] 
```

**Note:** As per the data dictionary, NAmes refers North Ames and NoRidge refers to Northridge both located in Iowa, US.

**Question 3:** What miscellaneous feature add the most value?

```{r}
# Which miscellaneous feature is the most prevalent?
misc <- df %>% 
            group_by(MiscFeature) %>%
            summarise(count = n_distinct(SalePrice)) %>% 
            arrange(desc(count))
            
misc[2,] 
```

**Takeaway:** For houses with miscellaneous features, Shed is the most prevalent in 46 houses.

```{r}
# Calculating the value added
misc_rows <- df %>% filter(MiscFeature == 'Shed')
avg_mon_val <- mean(misc_rows$MiscVal)
per_sale <- mean(misc_rows$MiscVal/misc_rows$SalePrice)*100
avg_mon_val
per_sale
```

**Takeaway:** Shed has on average \$697.84 of monetary value making up 0.48% of the house sale price on average.

## 5. Feature Engineering

**Feature Scaling:** When dealing with data, we are working with different types of which required adpated pre-processing before applying any machine learning techniques. In our content, we perform feature scaling to standardize only the values in continuous numerical variables. Read more [here](https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35).

```{r}
# Filter numeric columns
num_vars = c('LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1',
           'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',
           'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',
            'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal')

df[, num_vars] <- scale(df[, num_vars], center=TRUE, scale=TRUE)
df %>% head()
```

**Categorical feature encoding** ensures that variables with categories/groupings are transformed into numerical inputs for the predictive modeling phase. The categorical variables are also subdivided as:

-   binary (two possible outcomes)

-   cardinal (no meaningful order)

-   ordinal (meaningful order)

Read more [here](https://www.analyticsvidhya.com/blog/2020/08/types-of-categorical-data-encoding/).

```{r}
# List of nominal categorical variables
cat_vars = c('CentralAir', 'MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotConfig', 
            'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 
            'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 
            'Exterior2nd', 'MasVnrType', 'Foundation', 'Electrical', 
            'Functional', 'GarageType', 'MiscFeature', 'SaleType', 
            'SaleCondition', 'LotShape', 'LandContour', 'Utilities', 
            'LandSlope', 'OverallQual', 'OverallCond', 'ExterQual',
            'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure',
            'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual',
            'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond',
            'PavedDrive', 'PoolQC', 'Fence')

df[cat_vars] <- lapply(df[cat_vars], factor)
df %>% head()
```

**Datetime Variables:** There are variables denoting dates and thus, may hold significance and impact our target variable: the house's sale price.

Based on research, we thought that the most sensible option would be to transform the datetime variables into ordinal categories in twofold:

-   Direct encoding of 'MoSold' and 'YrSold' having 12 and 5 pre-defined categories that are the 12 months and 5 years respectively during which the houses in the dataset were sold.

-   Binning of 'YearRemodAdd' and 'YearBuilt' into 6 categories of 10 and 20 years of interval respectively before proceeding to ordinal encoding as well.

```{r}
df <- df %>% 
        mutate(YearRemodAdd = cut(YearRemodAdd, breaks=6),
               YearBuilt = cut(YearBuilt, breaks=6))
df %>% head()
```

```{r}
# List of date categorical variables
cat_vars = c('YearRemodAdd', 'YearBuilt', 'MoSold', 'YrSold')

df[cat_vars] <- lapply(df[cat_vars], factor)
df %>% head()
```

## 4. Correlation Analysis

```{r}
# Correlation matrix and plot
df_num <- df[, sapply(df, class) == "numeric"]
cor <- cor(df_num)
ggcorrplot(cor, hc.order = TRUE, insig = "blank",
           type = "lower", ggtheme = theme_gray,
           colors = c("#6D9EC1", "white", "#E46726"),
           tl.cex = 10)
```

**Note:** Next, we will only filter out relatively and highly correlated relationship with coefficient between 0.7 and 1 (non-inclusive to avoid pairs of identical variables).

```{r}
findCorrelation(x=cor, cutoff = .7, names=TRUE)
```

## 7. Machine Learning Set-Up

Under this section, we will explain the procedure of two main splitting approach to estimate our models' performance.

```{r}
## Training Testing Split
N <- nrow(df)
trainingSize  <- round(N*0.7)
trainingCases <- sample(N, trainingSize)
train <- df[trainingCases,]
test <- df[-trainingCases,]
```

```{r}
# K-Fold Cross Validation
train.control <- trainControl(method = "cv", number = 10, verboseIter = FALSE)
# Train the model
model <- train(SalePrice ~., data = df, method = "lm",
               trControl = train.control)

# Making predictions
# pred <- predict(model, train)
# obs <- train$SalePrice
# rmse(obs, pred)
```

**Takeaway:** Based on the printout above, it will inform us about a multicollinearity issue we need to solve. So, before building any new model, we will test various techniques to solve it.
