---
title: "Graduate Admission"
author: Akoua Orsot
date: September 2nd, 2022
version: 1.0
output: github_document
---

# Graduate Admission

In the course of a career, there arise multiple learning and professional opportunities like a Master's degree which often present itself as a fairly competitive race to the best universities and colleges around the world. Still, there are certain traits distinguishing applicants in the eye of admission offices. So, this notebook will attempt to build a predictive algorithm to determine chances of admissions (scale 0 to 1) given a set of variables. In doing so, we will follow LIME (Local Interpretable Model-agnostic Explanations) principles making it accessible and user-friendly to most readers.

## Table of Contents

1.  Environment set-up

    -   Importing Libraries

    -   Loading the data

2.  Initial Diagnostics

    -   Glimpse

    -   Descriptive Statistics

    -   Target Variable Analysis

    -   Predictors Analysis

3.  Data Processing

    -   Drop & Rename

    -   Missing Values

    -   Outliers

    -   Duplicate Observations

    -   Correlation Matrix

4.  Exploratory Data Analysis (EDA)

    -   What is the distribution of our continuous predictors?

    -   Is there a cluster of admitted (prob \>= 0.75) and non-admitted by GRE & TOEFL Scores?

    -   How does the Undergrad GPA affect Masters Program Admissions given research experience?

    -   Would the undergraduate's college/university strengthen application statements and recommendations?

5.  Feature Engineering

    -   Categorical Encoding

    -   Variable Standardization

6.  Machine Learning set-up

    -   Train-test split

    -   Cross-validation

7.  Machine Learning - Simple Models

    -   Logistic Regression

    -   k-Nearest Neighbors

    -   Decision Tree

    -   Stochastic Gradient Descent

8.  Machine Learning - Ensemble Methods

    -   Random Forest

    -   Stochastic Gradient Boosting

    -   StackingRegressor

9.  Trained Model Performance Evaluation

10. Hyperparameter Tuning

11. Final Model - Test Data Performance

## 1. Environment Set-up

```{r}
## Importing libraries
set.seed(1)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(ggpubr)

library(ROSE)
library(corrplot)

library(e1071)
library(caret)
library(rpart)

library(gmodels)
library(glmnet)

library(MLmetrics)
require(MASS)
library(PRROC)
```

```{r}
## Loading dataset
df <- read.csv(file = 'C:/Users/Graduate/Desktop/ds_proj_data/Admission_Predict_Ver1.1.csv')
```

## 2. Initial Diagnostics

```{r}
## Glimpse of the data
df %>% head()
```

**Note:** The dataset contains several parameters which are considered important during the application for Masters Programs. The parameters included are :

-   GRE Scores ( out of 340 )

-   TOEFL Scores ( out of 120 )

-   University Rating ( out of 5 )

-   Statement of Purpose and Letter of Recommendation Strength ( out of 5 )

-   Undergraduate GPA ( out of 10 )

-   Research Experience ( either 0 or 1 )

-   Chance of Admit ( ranging from 0 to 1 )

```{r}
## Descriptive Statistics
df %>% summary()
```

```{r}
# Target Variable Analysis
df %>% ggplot(aes(Chance.of.Admit)) +
  geom_histogram(bins=14) +
  labs(
  x = "Admission Probability",
  y = "Frequency (Count)",
  title= "Distribution Admission Probability"
 )
```

**Takeaway:** We have here a left-skewed distribution of admission chances among the pool of candidates. With most people sitting in the range of 0.6 to 0.8, we can assume that those students demonstrated comopentency in the various test scores and undergraduate studies.

## 3. Data Cleaning

**Note:** Before anything, we will proceed in dropping the Serial No. column and renaming some columns to retain ease of data manipulation.

```{r}
df <- df %>% 
        dplyr::select(-c(Serial.No.)) %>% 
        rename("target"="Chance.of.Admit") 
```

```{r}
## Missing Values
df %>% is.na() %>% sum()
```

```{r}
df %>% ggplot(aes(x=target)) +
  geom_boxplot()
```

```{r}
# Z score to identify outliers  
target <- df$target
z_scores_target <- abs((target-mean(target))/sd(target))

#display z-scores 
z_scores_target[z_scores_target >= 2.5]
```

**Takeaway:** From a visual standpoint, the boxplot only shows one clear outlier on the lower end of the distribution. However, when conducting the z-score calculation at ±2.5 away from the σ, we detected 4 outliers.

**Note:** Now, we will proceed in doing the same thing with each continuous independent variable

```{r}
#find z-scores of each column
all_zscores <- sapply(df, function(df) abs((df-mean(df))/sd(df)))

all_zscores[all_zscores >= 2.5]
```

**Takeaway:** As we thought about handling those outliers, we decided to keep them all since good data science practices advocates to conserve as many data points as possible. Thus, allowing us to limit the biases simply to produce a better fitting model or statistically significant results.

# 4. Exploratory Data Analysis

### What is the distribution of our target variable?

```{r}
df %>% ggplot(aes(x=target)) +
  geom_histogram() +
  xlab("Admission Probability") +
  ylab("Frequency (Count)") +
  labs(title="Distribution of Target Variable")
```

**Takeaway:** We have here a left-skewed distribution of admission chances among the pool of candidates. With most people sitting in the range of 0.6 to 0.8, we can assume that those students demonstrated comopentency in the various test scores and undergraduate studies.

### What is the distribution of our continuous predictors?

```{r}
sop_plot <- df %>% ggplot(aes(SOP)) +
                   geom_boxplot()

lor_plot <- df %>% ggplot(aes(LOR)) +
                   geom_boxplot()

cgpa_plot <- df %>% ggplot(aes(CGPA)) +
                   geom_boxplot()

comb_plot <- ggarrange(sop_plot, lor_plot, cgpa_plot, 
                       ncol = 2, nrow = 2)

annotate_figure(comb_plot, 
                top = text_grob("Distrbituion of continuous predictors", 
               color = "red", face = "bold", size = 14))
```

**Takeaway:** In the chart above, we can observe a fairly normal distribution across all three variables thus, reassuring us of the normality withing our data.

### Is there a cluster of admitted (prob \>= 0.75) and non-admitted by GRE & TOEFL Scores?

```{r}
# Making a categorical target varibale using a threshold
df['admin_binary'] <- as.factor(ifelse(target >= 0.75, 1, 0))

# Ploting the TOEFL & GRE Scores accordingly
toefl_hist <- df %>% ggplot(aes(x=TOEFL.Score, fill=admin_binary)) +
                     geom_density()

gre_hist <- df %>% ggplot(aes(x=GRE.Score, fill=admin_binary)) +
                     geom_density()

comb_plot <- ggarrange(toefl_hist, gre_hist, common.legend = TRUE,
                       ncol = 2, nrow = 1)

annotate_figure(comb_plot, 
                top = text_grob("TOEFL & GRE Scores by Admin Status", 
               color = "red", face = "bold", size = 14))
```

### How does the Undergrad GPA affect Masters Program Admissions given research experience?

```{r}
df %>% ggplot(aes(x=CGPA, y=target, color=Research)) +
  geom_point() +
  labs(title="Admission rate by College GPA based on Research Experience")
```

**Takeaway:** With the multi-plot shown of admission probability with respect to College GPA, there appearss to be a strong correlated relationship. Though they are not as clearly separated, the clusters of those with higher results and research experience stands a better chance of admission compared to their counterparts with no research experience and average to low GPA.

### Would the undergraduate's college/university strengthen application statements and recommendations?

```{r}
# sop_bar <- df %>% ggplot(aes(x=University.Rating, y=SOP, color=admin_binary)) +
#                    geom_bar()
# 
# lor_bar <- df %>% ggplot(aes(x=University.Rating, y=LOR, color=admin_binary)) +
#                    geom_bar()
# 
# comb_plot <- ggarrange(sop_bar, lor_bar, common.legend = TRUE,
#                        ncol = 2, nrow = 1)
# 
# annotate_figure(comb_plot, 
#                 top = text_grob("Application document strength based on University Rating", 
#                color = "red", face = "bold", size = 14))
```

**Takeaway:** In contrast to the clear seperation above, the university rating does not have a drastic effect on those predictors. It would indicates how much weight this variable has in the final decision process.
