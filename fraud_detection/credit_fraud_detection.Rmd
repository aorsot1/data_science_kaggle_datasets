---
title: "fraud_detection"
author: "Akoua Orsot"
date: "2/22/2022"
output:
  github_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Fraud Detection

This notebook will attempt to build a predictive algorithm to detect a fraudulent transaction using a training dataset. We will explain the thinking process at every step using LIME (Local Interpretable Model-agnostic Explanations) principles making it accessible and user-friendly.

## 1. Environment Set-up

```{r}
## Importing libraries
# set.seed(1)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(e1071)
library(caret)

library(corrplot)
library(ROSE)

library(rpart)
library(gmodels)
library(glmnet)
library(boot)
library(MLmetrics)
library(MLeval)
require(MASS)
library(Dict)
library(purrr) # for functional programming (map)
library(pROC) # for AUC calculations
library(PRROC) # for Precision-Recall curve calculations
```

```{r}
## Loading dataset
df <- read_csv(file = 'C:/Users/Akoua Orsot/Desktop/ds_projects_data/creditcard.csv')
```

## 2. Initial Diagnostics

```{r}
## Glimpse of the data
head(df)
```

```{r}
## Descriptive Statistics
summary(df)
```

**Takeaway:** The following percentage breakdown confirms the note in the project description; indeed, we have a considerable class imbalance with the target variable. It stays consistent that most fraudulent activities are much less frequent than non-fraudulent. Before proceeding, we shall note it to avoid any overfitting issues when fitting the machine learning models.

```{r}
## Target Variable Analysis
df %>% group_by(Class) %>%
  summarise(cnt = n()) %>%
  mutate(freq = round(cnt / sum(cnt), 5)) %>% 
  arrange(desc(freq))
```

**Note:** We did not have any information on the numerical predictors for privacy, given their transformation and standardization, excluding Amount & Time. In that regard, Amount presented itself as potentially most informative for the feature variable analysis. To better understand the variable's distribution, we had to transform it using a log scale.

```{r}
## Target Variable Analysis
df$Amount %>% summary()

df %>% ggplot(aes(Amount)) +
  geom_histogram(bins=35) +
  scale_x_log10() +
  labs(
  x = "Dollar Amount (Log Scale)",
  y = "Frequency (Count)",
  title= "Distribution of Transaction Amount (log scaled)"
 )
```

## 3. Data Cleaning

```{r}
## Missing Values
df %>% is.na() %>% sum()
```

**Takeaway:** As the count shows, we have no missing values given the pre-processing done prior.

**Note**: With most predictors transformed, there will be little chance for any outliers in the data points for V1, V2, ..., V28. So, we will only examine Amount as the only meaningful numeric feature.

```{r}
df %>% ggplot(aes(x=Amount)) +
  geom_boxplot() +
  labs(
  x = "Amount ($)",
  title= "Distribution of Transaction Amount"
 )
```

**Takeaway:** From the boxplot below, we can observe a non-negligible number of outliers on the upper end of the distribution. It would denote transactions with high amounts in the order of thousands of dollars. We would assess the effect of this skewed distribution when building the predictive models in terms of feature transformation or selecting models robust to such feature types.

```{r}
df %>% duplicated() %>% sum()
```

**Takeaway:** A quick check reveals 1081 duplicate rows, so we proceed in removing them from the dataset.

```{r}
df <- df[!duplicated(df), ]
```

**Definition:** Feature Engineering

```{r}
df$Amount <- scale(df$Amount)
```

## 4. Correlation Analysis

```{r}
df_cor <- cor(df)
corrplot(df_cor, method = 'color')
```

**Takeaway:** From the correlation matrix plotted, we can observe very few correlated variables as we would expect after the feature transformation. The two meaningful features, are Time and Amount, have some relative correlation with some variables with coefficients approximating 0.4. With such low values, it would be pretty challenging to imply a correlation between any of them with any certainty. It also indicates that there would be a very low incidence of any colinearity within our data

**Note:** The code below filters those pairs with correlation coefficients above 0.5 as a threshold. As noted above, those values give very little to no confidence in any solid correlated relationship between variables as few crossing the 0.5 mark.

```{r}
df_cor <- as.data.frame(df_cor)
df_cor[(abs(df_cor) >= 0.5) & (abs(df_cor) !=1)]

```

## 5. Inquiry Exploration

**Note:** In an attempt to answer the first question, we first split our dataset by class types; in other words, fraudulent and non-fraudulent transactions. We then plot the histogram side by side to observe any unusual behavior. In doing so, the non-fraud transactions were heavily right-skewed, making it quite challenging to compare the plots. To solve this issue, we used a logarithmic transformation, making it easier to see and thus, evaluate any similarities and differences.

```{r}
# How does Amount's distribution behaves across classes?

# Splitting data by fraud class
df_no_fraud <- df %>% filter(Class == 0)
df_fraud <- df %>% filter(Class == 1)

# Histogram for Amount Distribution per class
df_no_fraud %>% ggplot(aes(x=Amount)) +
  geom_histogram(color="black", fill="white", bins=100) +
  labs(
  x = "Scaled Amount",
  title= "Distribution of Non-Fraud Transactions"
 )

df_fraud %>% ggplot(aes(x=Amount)) +
  geom_histogram(color="black", fill="white", bins=50) +
  labs(
  x = "Scaled Amount",
  title= "Distribution of Fraud Transactions"
 )

```

**Takeaway:** Before making a note on the plots, we will first explain how to interpret logarithmic scales. In short, log scales show relative values rather than absolute ones. Indeed, 2 minus 1 would be displayed similarly to 9999 minus 9998, given that we are dealing with percentages here. In context, the histograms below would depict the order of growth of transaction value. Both distributions represent a similar trajectory, with most transactions on the lower end of the graph. It stays consistent with the mean value found at USD88, even with max values averaging USD20,000.

**Note:** For the second question, we will check the timing of transactions to detect anything unusual. We will use only the fraud dataset and plot a scatterplot accordingly.

```{r}
## Are there any noteworthy point in time where fraud occured?
# Scatterplot
df_fraud %>% ggplot(aes(x=Time, y=Amount)) +
  geom_point() +
  labs(
  y = "Amount ($)", 
  x = "Time (s)",
  title= "Fraudulent Transactions Across Time"
 )

```

**Takeaway:** The graph above does not appear that there is a clustering pattern on a time interval. So, we would assume that fraud occurred across time quite randomly.

## 6. Class Imbalance

**Note:** Our diagnostics observed a stark imbalance between classes of transactions, with fraud only making up 0.2% of all transaction statuses. Given the limited pool of examples to train, it poses an issue in terms of building an effective machine model to predict if there is a fraud. With the minority class being so small, we would expect poor performance on the critical task of detecting fraud transactions. In that vein, we will use different sampling methods (Undersampling & Oversampling) to tackle this problem.

**Definition:** SMOTE (Synthetic Minority Oversampling Technique) is an oversampling approach to the minority class. In context, it would mean to randomly increase fraud examples by "artificially" replicating to have a more balanced class distribution. Further information [here](https://rikunert.com/smote_explained).

```{r}
## now using ROSE for oversampling
ROSE_over <- ovun.sample(Class ~., data=df,
                                  p=0.5, seed=1,
                                  method="over")

data_balanced_over <- ROSE_over$data
```

```{r}
## Check class distribution after using SMOTE
data_balanced_over %>% group_by(Class) %>%
  summarise(cnt = n()) %>%
  mutate(freq = round(cnt / sum(cnt), 5)) %>% 
  arrange(desc(freq))
```

**Definition:** We proceed to an undersampling approach on the majority class. In context, we select examples to keep out of the training set based on the distance of majority class examples to minority class examples. Further information [here](https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/).

```{r}
## now using ROSE for oversampling
ROSE_under <- ovun.sample(Class ~., data=df,
                                  p=0.5, seed=1,
                                  method="under")

data_balanced_under <- ROSE_under$data
```

```{r}
## Check class distribution after using SMOTE
data_balanced_over %>% group_by(Class) %>%
  summarise(cnt = n()) %>%
  mutate(freq = round(cnt / sum(cnt), 5)) %>% 
  arrange(desc(freq))
```

**Note:** With the risk of overfitting with oversampling and the possibility of losing valuable information from undersampling, we understand that we cannot have a perfect solution to this class imbalance problem. In that vein, we shall proceed with the oversampling approach by building robust models to avoid overfitting.

```{r}
df <- data_balanced_over
df
```

## 7. Machine Learning set-up

Under this section, we will explain the procedure of two main splitting approach to estimate our models' performance.

**Definition:** Often denoted as the most popular by its simplicity, the train-test split is a sampling technique dividing the dataset between training and testing sets. In doing so, the goal would be to have enough (but not too much) in our training set used for the machine learning model to predict the observations in the testing set as accurately as possible. Most would opt for a 70/30 training-testing split, respectively, others 80/20, 60/40, or whichever else works best for the case scenario. Further information [here](https://machinelearningmastery.com/train-test-split-for-evaluating-machine-learning-algorithms/).

```{r}
# Setting Target Variable to categorical
df$Class <- as.factor(df$Class)
```

```{r}
## Training Testing Split
N <- nrow(df)
trainingSize  <- round(N*0.7)
trainingCases <- sample(N, trainingSize)
train <- df[trainingCases,]
test <- df[-trainingCases,]
```

```{r}
# Building tree model
tree <- rpart(Class ~ ., data = train, method = "class")

# Making predictions
pred <- predict(tree, train, type="class")
obs <- train$Class
acc <- 1-sum(pred != obs)/nrow(train)
acc
```

**Definition:** As the name would suggest, we will engage here in the process of validation to ensure reliability on our model. Cross-Validation is a statistical method applied in various ways to estimate the model's performance. Some examples are **Holdout Method, K-Fold, Stratified K-Fold, Leave-P-Out.** Further information [here](https://machinelearningmastery.com/k-fold-cross-validation/) and [here](https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f).

**Note:** As we already tackled the issue of class imbalance with a combination of under- and over-sampling, we will use the K-Fold cross-validation

```{r}
# K-fold cross-validation 
# Building tree model
tree_cv <- rpart(Class ~ ., data = train,
              method = "class", control = rpart.control(xval=5))
plotcp(tree_cv)
```

## 8. Dimensionality Reduction

This section will use dimensionality reduction to trim down the number of features we have. Dimensionality reduction encapsulates the techniques reducing the input variables in our training data. In doing so, we hope to have a more straightforward but effective machine learning model structure and avoid any potential case of overfitting. We will be testing three different methods from Linear Algebra: **PCA, SVD, and LDA** and pick the one capturing the most variability in the datasets after reducing it to principal components. We performed the evaluation and comparison process using the following [code](https://rpubs.com/JanpuHou/278584) from Janpu Hou on their RPubs blogs.

```{r}
# Seperating features from target vari
X <- subset(df, select = -c(Class))
```

```{r}
# Function to check Variance explained
pcaCharts <- function(x) {
    x.var <- x$sdev ^ 2
    x.pvar <- x.var/sum(x.var)
    print("proportions of variance:")
    print(x.pvar)
    
    par(mfrow=c(2,2))
    plot(x.pvar,
         xlab="Principal component", 
         ylab="Proportion of variance explained", 
         ylim=c(0,1), type='b')
    plot(cumsum(x.pvar),
         xlab="Principal component", 
         ylab="Cumulative Proportion of variance explained", 
         ylim=c(0,1), type='b')
    screeplot(x)
    screeplot(x,type="l")
    par(mfrow=c(1,1))
}
```

**Definition:** PCA (Principal Component Analysis) takes data with m-columns projected to a subspace with n-features (n \< m) while preserving the crucial information from the original data; in other words, PCA attempts to find the **principal components (or features)** as its names denote. Further information [here](https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/).

```{r}
pca <- prcomp(X, scale. = TRUE)
# pcaCharts(pca)

# Calculate Explained Variance
pca_var <- round((pca$sdev^2)/sum(pca$sdev^2) * 100, 1) 
n_pc <- seq(1, length(pca_var))
pca_df <- as.data.frame(pca_var, n_pc)

# Graph of explained variance
pca_df %>% 
  ggplot(aes(x = n_pc, y = pca_var)) +
    geom_bar(stat = 'identity') +
    labs(title = "PCA Variation Plot",
         xlab = "PCs",
         ylab = "Prop. of variance explained",
         ylim = c(0, 100))
```

**Definition:** SVD (Singular Value Decomposition) is a process breaking down a matrix into its constituents elements by factorizing it into three separate matrices: **M=UΣVᵗ**.

-   M: original matrix

-   U: left singular matrix (columns are left singular vectors) containing eigenvectors of matrix MMᵗ

-   Σ: a diagonal matrix containing singular (eigen)values

-   V: right singular matrix (columns are right singular vectors) containing eigenvectors of matrix MᵗM.

Further information [here](https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/).

```{r}
svd <- svd(X)

# Calculate Explained Variance
svd_var <- round(svd$d^2/sum(svd$d^2) * 100, 4) 
n_pc <- seq(1, length(svd_var))
svd_df <- as.data.frame(svd_var, n_pc)

# Graph of explained variance
svd_df %>% 
  ggplot(aes(x = n_pc, y = svd_var)) +
    geom_bar(stat = 'identity') +
    labs(title = "SVD Variation Plot",
         xlab = "PCs",
         ylab = "Prop. of variance explained",
         ylim = c(0, 100))
```

**Takeaway:** Beyond 5 principal components, there is very little improvement in the explained variance for PCA while the first column explained the majority of the variance using SVD. We will move forward with the reduced dataframe from PCA proving more stable.

```{r}
# Consolodiating PCA transformed dataframe
X_pca <- as.data.frame(pca$x[, 1:5])
y <- subset(df, select = c(Class))
df_pca <- cbind(X_pca, y)
```

```{r}
# Partitioning train-test split
index <- createDataPartition(y = df_pca$Class, p = .7, 
                             times = 1, list = FALSE)
train <- df_pca[index,]
test  <- df_pca[-index,]
```

## 9. Machine Learning - Simple Models

This section will leverage the powerful sci-kit-learn package to build multiple models with little to no parameter tuning for comparison. We will only use the cross-validation error on our training dataset to avoid any data leakage.

```{r}
# K-fold cross validation
kfold_cv <- trainControl(method = "cv",  number = 3, 
                        savePredictions = "all"
                        )
```

**Definition:** Logistic Regression is a predictive classifier that models an S-shaped curve (Sigmoid function) on the data to label the examples. Further information [here](https://machinelearningmastery.com/logistic-regression-for-machine-learning/).

```{r}
model <- train(form=Class~., data = train,
               method="glm", family="binomial",
               trControl=kfold_cv, tuneLength=3)

preds <- predict(model, train, type="prob")[,2] #prob of positive class
preds_pos <- preds[train[,6]==1] #preds for true positive class
preds_neg <- preds[train[,6]==0] #preds for true negative class

PRC <- pr.curve(preds_pos, preds_neg, curve=TRUE)
plot(PRC)
```

**Definition:** A Decision Tree is a supervised machine learning algorithm building an actual tree based on splits within the data. [here](https://www.xoriant.com/blog/product-engineering/decision-trees-machine-learning-algorithm.html).

```{r}
model <- train(form=Class~., data = train,
               method="rpart", 
               trControl=kfold_cv, tuneLength=3)
preds <- predict(model, train, type="prob")[,2] #prob of positive class
preds_pos <- preds[train[,6]==1] #preds for true positive class
preds_neg <- preds[train[,6]==0] #preds for true negative class

PRC <- pr.curve(preds_pos, preds_neg, curve=TRUE)
plot(PRC)
```

**Definition:** Stochastic Gradient Descent is an iterative algorithm that minimizes the model's error rate. Further information [here](https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31).

```{r}
sgd_model <- train(form=Class~., data = train,
               method="gbm", verbose = FALSE,
               trControl=kfold_cv, tuneLength=3)

preds <- predict(model, train, type="prob")[,2] #prob of positive class
preds_pos <- preds[train[,6]==1] #preds for true positive class
preds_neg <- preds[train[,6]==0] #preds for true negative class

PRC <- pr.curve(preds_pos, preds_neg, curve=TRUE)
plot(PRC)
```

**Takeaway:** Our best model is the Logistic regression at 96.2% AUPRC.

# 10. Machine Learning - Ensemble Methods

This section will extend our work in machine learning to incorporate ensemble methods. We generated simple models and compared the scores, which appear satisfactory, with the lowest at 0.974 for the Average Precision Score. However, we may want more stability and minor variation in our predictive algorithm; it is where ensemble techniques come in. Most often, they act as a 'superposer' of multiple models throughout various ways and thus, bolster their predictive power. Further Information [here](https://machinelearningmastery.com/tour-of-ensemble-learning-algorithms/).

```{r}

```
