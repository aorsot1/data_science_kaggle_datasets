---
title: "fraud_detection"
author: "Akoua Orsot"
date: "2/14/2022"
output:
  github_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Fraud Detection

This notebook will attempt to build a predictive algorithm to detect a fraudulent transaction using a training dataset. We will explain the thinking process at every step using LIME (Local Interpretable Model-agnostic Explanations) principles making it accessible and user-friendly.

## 1. Environment Set-up

```{r}
## Importing libraries
set.seed(1)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(e1071)
library(caret)


library(corrplot)
library(ROSE)
library(hyperSMURF)

library(rpart)
library(gmodels)
library(glmnet)
library(boot)
```

```{r}
## Loading dataset
df <- read_csv(file = 'C:/Users/Akoua Orsot/Desktop/ds_projects_data/creditcard.csv')
head(df)
```

## 2. Initial Diagnostics

```{r}
## Glimpse of the data
df %>% str()
```

```{r}
## Descriptive Statistics
df %>% summary()
```

**Takeaway:** The following percentage breakdown confirms the note in the project description; indeed, we have a considerable class imbalance with the target variable. It stays consistent that most fraudulent activities are much less frequent than non-fraudulent. Before proceeding, we shall note it to avoid any overfitting issues when fitting the machine learning models.

```{r}
## Target Variable Analysis
df %>% group_by(Class) %>%
  summarise(cnt = n()) %>%
  mutate(freq = round(cnt / sum(cnt), 5)) %>% 
  arrange(desc(freq))
```

**Note:** We did not have any information on the numerical predictors for privacy, given their transformation and standardization, excluding Amount & Time. In that regard, Amount presented itself as potentially most informative for the feature variable analysis. To better understand the variable's distribution, we had to transform it using a log scale.

```{r}
## Target Variable Analysis
df$Amount %>% summary()

df %>% ggplot(aes(Amount)) +
  geom_histogram(bins=35) +
  scale_x_log10() +
  labs(
  x = "Dollar Amount (Log Scale)",
  y = "Frequency (Count)",
  title= "Distribution of Transaction Amount (log scaled)"
 )
```

## 3. Data Cleaning

```{r}
## Missing Values
df %>% is.na() %>% sum()
```

**Takeaway:** As the count shows, we have no missing values given the pre-processing done prior.

**Note**: With most predictors transformed, there will be little chance for any outliers in the data points for V1, V2, ..., V28. So, we will only examine Amount as the only meaningful numeric feature.

```{r}
df %>% ggplot(aes(x=Amount)) +
  geom_boxplot() +
  labs(
  x = "Amount ($)",
  title= "Distribution of Transaction Amount"
 )
```

**Takeaway:** From the boxplot below, we can observe a non-negligible number of outliers on the upper end of the distribution. It would denote transactions with high amounts in the order of thousands of dollars. We would assess the effect of this skewed distribution when building the predictive models in terms of feature transformation or selecting models robust to such feature types.

```{r}
df %>% duplicated() %>% sum()
```

**Takeaway:** A quick check reveals 1081 duplicate rows, so we proceed in removing them from the dataset.

```{r}
df <- df[!duplicated(df), ]
```

**Definition:** Feature Engineering

```{r}
df$Amount <- scale(df$Amount)
```

## 4. Correlation Analysis

```{r}
df_cor <- cor(df)
corrplot(df_cor, method = 'color')
```

**Takeaway:** From the correlation matrix plotted, we can observe very few correlated variables as we would expect after the feature transformation. The two meaningful features, are Time and Amount, have some relative correlation with some variables with coefficients approximating 0.4. With such low values, it would be pretty challenging to imply a correlation between any of them with any certainty. It also indicates that there would be a very low incidence of any colinearity within our data

**Note:** The code below filters those pairs with correlation coefficients above 0.5 as a threshold. As noted above, those values give very little to no confidence in any solid correlated relationship between variables as few crossing the 0.5 mark.

```{r}
df_cor <- as.data.frame(df_cor)
df_cor[(abs(df_cor) >= 0.5) & (abs(df_cor) !=1)]

```

## 5. Inquiry Exploration

**Note:** In an attempt to answer the first question, we first split our dataset by class types; in other words, fraudulent and non-fraudulent transactions. We then plot the histogram side by side to observe any unusual behavior. In doing so, the non-fraud transactions were heavily right-skewed, making it quite challenging to compare the plots. To solve this issue, we used a logarithmic transformation, making it easier to see and thus, evaluate any similarities and differences.

```{r}
# How does Amount's distribution behaves across classes?

# Splitting data by fraud class
df_no_fraud <- df %>% filter(Class == 0)
df_fraud <- df %>% filter(Class == 1)

# Histogram for Amount Distribution per class
df_no_fraud %>% ggplot(aes(x=Amount)) +
  geom_histogram(color="black", fill="white", bins=100) +
  labs(
  x = "Scaled Amount",
  title= "Distribution of Non-Fraud Transactions"
 )

df_fraud %>% ggplot(aes(x=Amount)) +
  geom_histogram(color="black", fill="white", bins=50) +
  labs(
  x = "Scaled Amount",
  title= "Distribution of Fraud Transactions"
 )

```

**Takeaway:** Before making a note on the plots, we will first explain how to interpret logarithmic scales. In short, log scales show relative values rather than absolute ones. Indeed, 2 minus 1 would be displayed similarly to 9999 minus 9998, given that we are dealing with percentages here. In context, the histograms below would depict the order of growth of transaction value. Both distributions represent a similar trajectory, with most transactions on the lower end of the graph. It stays consistent with the mean value found at USD88, even with max values averaging USD20,000.

**Note:** For the second question, we will check the timing of transactions to detect anything unusual. We will use only the fraud dataset and plot a scatterplot accordingly.

```{r}
## Are there any noteworthy point in time where fraud occured?
# Scatterplot
df_fraud %>% ggplot(aes(x=Time, y=Amount)) +
  geom_point() +
  labs(
  y = "Amount ($)", 
  x = "Time (s)",
  title= "Fraudulent Transactions Across Time"
 )

```

**Takeaway:** The graph above does not appear that there is a clustering pattern on a time interval. So, we would assume that fraud occurred across time quite randomly.

## 6. Class Imbalance

**Note:** Our diagnostics observed a stark imbalance between classes of transactions, with fraud only making up 0.2% of all transaction statuses. Given the limited pool of examples to train, it poses an issue in terms of building an effective machine model to predict if there is a fraud. With the minority class being so small, we would expect poor performance on the critical task of detecting fraud transactions. In that vein, we will use different sampling methods (Undersampling & Oversampling) to tackle this problem.

**Definition:** SMOTE (Synthetic Minority Oversampling Technique) is an oversampling approach to the minority class. In context, it would mean to randomly increase fraud examples by "artificially" replicating to have a more balanced class distribution. Further information [here](https://rikunert.com/smote_explained).

```{r}
## now using ROSE for oversampling
ROSE_over <- ovun.sample(Class ~., data=df,
                                  p=0.5, seed=1,
                                  method="over")

data_balanced_over <- ROSE_over$data
```

```{r}
## Check class distribution after using SMOTE
data_balanced_over %>% group_by(Class) %>%
  summarise(cnt = n()) %>%
  mutate(freq = round(cnt / sum(cnt), 5)) %>% 
  arrange(desc(freq))
```

**Definition:** We proceed to an undersampling approach on the majority class. In context, we select examples to keep out of the training set based on the distance of majority class examples to minority class examples. Further information [here](https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/).

```{r}
## now using ROSE for oversampling
ROSE_under <- ovun.sample(Class ~., data=df,
                                  p=0.5, seed=1,
                                  method="under")

data_balanced_under <- ROSE_under$data
```

```{r}
## Check class distribution after using SMOTE
data_balanced_over %>% group_by(Class) %>%
  summarise(cnt = n()) %>%
  mutate(freq = round(cnt / sum(cnt), 5)) %>% 
  arrange(desc(freq))
```

**Note:** With the risk of overfitting with oversampling and the possibility of losing valuable information from undersampling, we understand that we cannot have a perfect solution to this class imbalance problem. In that vein, we shall proceed with the oversampling approach by building robust models to avoid overfitting.

```{r}
df <- data_balanced_over
df
```

## 7. Machine Learning set-up

Under this section, we will explain the procedure of two main splitting approach to estimate our models' performance.

**Definition:** Often denoted as the most popular by its simplicity, the train-test split is a sampling technique dividing the dataset between training and testing sets. In doing so, the goal would be to have enough (but not too much) in our training set used for the machine learning model to predict the observations in the testing set as accurately as possible. Most would opt for a 70/30 training-testing split, respectively, others 80/20, 60/40, or whichever else works best for the case scenario. Further information [here](https://machinelearningmastery.com/train-test-split-for-evaluating-machine-learning-algorithms/).

```{r}
# Setting Target Variable to categorical
df$Class <- as.factor(df$Class)
```

```{r}
## Training Testing Split
N <- nrow(df)
trainingSize  <- round(N*0.7)
trainingCases <- sample(N, trainingSize)
train <- df[trainingCases,]
test <- df[-trainingCases,]

mylogit <- glm(formula = train$Class ~ ., data = train, 
               family = "binomial")

summary(mylogit)
```

**Definition:** As the name would suggest, we will engage here in the process of validation to ensure reliability on our model. Cross-Validation is a statistical method applied in various ways to estimate the model's performance. Some examples are **Holdout Method, K-Fold, Stratified K-Fold, Leave-P-Out.** Further information [here](https://machinelearningmastery.com/k-fold-cross-validation/) and [here](https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f).

**Note:** As we already tackled the issue of class imbalance with a combination of under- and over-sampling, we will use the K-Fold cross-validation

```{r}
# K-fold cross-validation 
```
